{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abddf18a-23c4-4684-a3cb-d6aa4cfc4cbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -q transformers\n",
    "!python -m pip install -q pillow\n",
    "!python -m pip install -q loguru\n",
    "!python -m pip install -q pydantic\n",
    "!python -m pip install -q pandas\n",
    "!python -m pip install -q opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a195d6-ce49-4b92-add1-6aa200c422cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01709bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98395948-5a69-451b-98b5-be1c2fb197ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "399e333d-a46c-4cbb-a76b-0fbf7f01e3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-large-patch14\")\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-large-patch14\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "918ae3a4-4e34-49da-b8a6-be114c53d734",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OwlViTForObjectDetection(\n",
       "  (owlvit): OwlViTModel(\n",
       "    (text_model): OwlViTTextTransformer(\n",
       "      (embeddings): OwlViTTextEmbeddings(\n",
       "        (token_embedding): Embedding(49408, 768)\n",
       "        (position_embedding): Embedding(16, 768)\n",
       "      )\n",
       "      (encoder): OwlViTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x OwlViTEncoderLayer(\n",
       "            (self_attn): OwlViTAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): OwlViTMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (vision_model): OwlViTVisionTransformer(\n",
       "      (embeddings): OwlViTVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(3601, 1024)\n",
       "      )\n",
       "      (pre_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): OwlViTEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x OwlViTEncoderLayer(\n",
       "            (self_attn): OwlViTAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): OwlViTMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "    (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       "  )\n",
       "  (class_head): OwlViTClassPredictionHead(\n",
       "    (dense0): Linear(in_features=1024, out_features=768, bias=True)\n",
       "    (logit_shift): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    (logit_scale): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "  )\n",
       "  (box_head): OwlViTBoxPredictionHead(\n",
       "    (dense0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (dense2): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  )\n",
       "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953acdb9-1ce8-4ce5-97a1-f80f5865ddb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86338b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from statistics import mean\n",
    "import time\n",
    "import torch\n",
    "from utils.protocols import OwlVit\n",
    "from utils.utils import convert_model_detection\n",
    "from utils.video import read_vid_batch, read_video\n",
    "from utils.utils import get_file_name, get_gpu_name\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "\n",
    "BATCH_SIZE = 15\n",
    "\n",
    "MODEL= f\"owlvit-large-p14-hf-batch-{BATCH_SIZE}\"\n",
    "\n",
    "text = \"face\"\n",
    "BASE_DIR = \"experiments/owlvit\"\n",
    "\n",
    "\n",
    "def process_video(video):\n",
    "    start_time = datetime.now()\n",
    "    target_sizes = None\n",
    "    results = {}\n",
    "    pre_processing_times = []\n",
    "    inference_times = []\n",
    "    post_processing_times = []\n",
    "    last = None\n",
    "    for batch in read_vid_batch(video, batch_size=BATCH_SIZE):\n",
    "        start = time.time()\n",
    "        frame_ids = list(batch.keys())\n",
    "        frames =  [Image.fromarray(frame.astype(\"uint8\")) for frame in batch.values()]\n",
    "        prompts = [text for _ in frames]\n",
    "        target_sizes = [frame.size for frame in frames]\n",
    "        inputs = processor(images=frames, text=prompts, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        pre_processing_times.append(time.time()-start)\n",
    "        \n",
    "        start = time.time()\n",
    "        #with torch.no_grad():\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**inputs)\n",
    "        inference_times.append(time.time()-start)\n",
    "        \n",
    "        start = time.time()\n",
    "        detections = processor.post_process_object_detection(outputs=outputs, threshold=0.1, target_sizes=target_sizes)\n",
    "        batch_result =  {frame_id: convert_model_detection(detection) for frame_id, detection in zip(frame_ids, detections)}\n",
    "        results = {**results, **batch_result}\n",
    "        post_processing_times.append(time.time()-start)\n",
    "        torch.cuda.empty_cache()\n",
    "        last = frame_ids[-1]\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    n_frames = len(results)\n",
    "    \n",
    "    exp =  OwlVit(\n",
    "        model=MODEL,\n",
    "        gpu=get_gpu_name(),\n",
    "        video_file=video,\n",
    "        frames=None,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_frames=n_frames,\n",
    "        \n",
    "        pre_processing_time=sum(pre_processing_times),\n",
    "        inference_time=sum(inference_times),\n",
    "        post_processing_time=sum(post_processing_times),\n",
    "        video_processing_time=(end_time-start_time).seconds,\n",
    "       \n",
    "        start_time=start_time.isoformat(),\n",
    "        end_time=end_time.isoformat(),\n",
    "        record_file=get_file_name(BASE_DIR, start, MODEL, video),\n",
    "        \n",
    "        data=results,\n",
    "        prompt=text,\n",
    "        \n",
    "    )\n",
    "    exp.save()\n",
    "    exp.log()\n",
    "    torch.cuda.empty_cache()\n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b02ff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-06-12 11:31:00.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mutils.protocols\u001b[0m:\u001b[36mlog\u001b[0m:\u001b[36m113\u001b[0m - \u001b[1mdata/720.mp4 | frames=283 | model_fps=3.668199261126982 | inference_time=77.14957118034363 | preprocess_time=18.242178440093994\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(1):\n",
    "    vid_hd = process_video(\"data/720.mp4\")\n",
    "    results.append(vid_hd)\n",
    "    vid_fhd = process_video(\"data/1080.mp4\")\n",
    "    results.append(vid_fhd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c3730-114e-44b3-95ee-5286a6fe2bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = results[0].columns\n",
    "rows = [result.row for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910ba66-51ff-41f5-96a1-1447cb7b1bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93aed5-c053-4b96-a3fc-42e19263e361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "csv_file = f\"zz-{MODEL}-{get_gpu_name()}-{now.day}-{now.hour}-{now.minute}.csv\"\n",
    "df.to_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3a7d3c-aaca-4601-805f-a296c19136d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
